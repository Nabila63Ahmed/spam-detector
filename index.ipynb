{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filesystem access\n",
    "import os\n",
    "# for Unix filename pattern matching\n",
    "import fnmatch\n",
    "# for data analysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "# for natural language processing\n",
    "import nltk.tokenize as tokenizer\n",
    "from nltk import pos_tag, word_tokenize, sent_tokenize\n",
    "# for regular expression operations\n",
    "import re\n",
    "# for zipping lists\n",
    "from functools import reduce\n",
    "# for computing word syllables\n",
    "import pyphen\n",
    "# for spelling mistakes\n",
    "import enchant\n",
    "# for utilities\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINGSPAM_BARE_DATASET_PATH = \"datasources/lingspam/bare\"\n",
    "SPAM_TERM_LIST_PATH = \"datasources/wcling/spam-term-list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_spam_file_name(file_name):\n",
    "    return fnmatch.fnmatchcase(file_name, 'spmsg*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the emails in the ten folders & save the labels (spam/not spam, or 0/1) of each email to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, file_names in os.walk(LINGSPAM_BARE_DATASET_PATH):\n",
    "    for file_name in fnmatch.filter(file_names, '*.txt'):\n",
    "        with open(os.path.join(root, file_name), 'r') as file:\n",
    "            documents.append(file.read())\n",
    "            labels.append(1 if is_spam_file_name(file_name) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Read 2893 documents\n"
     ]
    }
   ],
   "source": [
    "documents_length = len(documents)\n",
    "\n",
    "if documents_length > 0:\n",
    "    print(\"✅ Read %i documents\" % len(documents))\n",
    "else:\n",
    "    print(\"❌ Could not read any documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the emails & labels into 80% training & 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents_count = round(documents_length * 0.8)\n",
    "\n",
    "training_documents = documents[:training_documents_count]\n",
    "training_labels = labels[:training_documents_count]\n",
    "\n",
    "testing_documents = documents[training_documents_count:]\n",
    "testing_labels = labels[training_documents_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the training emails & transform the testing emails using a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(training_documents)\n",
    "\n",
    "training_document_term_matrix = count_vectorizer.transform(training_documents)\n",
    "testing_document_term_matrix = count_vectorizer.transform(testing_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Naive Bayes classifier precision score: 0.975248\n",
      "🔎 Naive Bayes classifier recall score: 0.994824\n",
      "🔎 Naive Bayes classifier f-score: 0.984708\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = MultinomialNB(alpha=1)\n",
    "naive_bayes_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "naive_bayes_classifier_predictions = naive_bayes_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "naive_bayes_classifier_precision_score = metrics.precision_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_recall_score = metrics.recall_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_f1_score = metrics.f1_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 Naive Bayes classifier precision score: %f\" % naive_bayes_classifier_precision_score)\n",
    "print(\"🔎 Naive Bayes classifier recall score: %f\" % naive_bayes_classifier_recall_score)\n",
    "print(\"🔎 Naive Bayes classifier f-score: %f\" % naive_bayes_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 K Neighbors classifier precision score: 0.912112\n",
      "🔎 K Neighbors classifier recall score: 0.955325\n",
      "🔎 K Neighbors classifier f-score: 0.931835\n"
     ]
    }
   ],
   "source": [
    "kneighbors_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "kneighbors_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "kneighbors_classifier_predictions = kneighbors_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "kneighbors_classifier_precision_score = metrics.precision_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_recall_score = metrics.recall_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_f1_score = metrics.f1_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 K Neighbors classifier precision score: %f\" % kneighbors_classifier_precision_score)\n",
    "print(\"🔎 K Neighbors classifier recall score: %f\" % kneighbors_classifier_recall_score)\n",
    "print(\"🔎 K Neighbors classifier f-score: %f\" % kneighbors_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Random Forest classifier precision score: 0.970471\n",
      "🔎 Random Forest classifier recall score: 0.879173\n",
      "🔎 Random Forest classifier f-score: 0.917266\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=0)\n",
    "random_forest_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "random_forest_classifier_predictions = random_forest_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "random_forest_classifier_precision_score = metrics.precision_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_recall_score = metrics.recall_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_f1_score = metrics.f1_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 Random Forest classifier precision score: %f\" % random_forest_classifier_precision_score)\n",
    "print(\"🔎 Random Forest classifier recall score: %f\" % random_forest_classifier_recall_score)\n",
    "print(\"🔎 Random Forest classifier f-score: %f\" % random_forest_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying using Readability Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_word_tokenized = [word_tokenize(document) for document in documents]\n",
    "documents_sentence_tokenized =  [sent_tokenize(document) for document in documents]\n",
    "documents_word_tagged = [pos_tag(document_word_tokenized) for document_word_tokenized in documents_word_tokenized]\n",
    "\n",
    "spam_term_list = list(filter(lambda x: x, open(SPAM_TERM_LIST_PATH, \"r\").read().split('\\n')))\n",
    "spam_term_list = [spam_sentence.lower() for spam_sentence in spam_term_list]                  \n",
    "\n",
    "dictionary = pyphen.Pyphen(lang='en_GB')\n",
    "documents_syllabafied = list(map(lambda document_word_tokenized: list(map(lambda word: len(dictionary.inserted(word).split('-')), document_word_tokenized)), documents_word_tokenized))\n",
    "\n",
    "spelling_dictionary = enchant.Dict('en_US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1: The number of sentences in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = [len(document_sentence_tokenized) for document_sentence_tokenized in documents_sentence_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Number of sentences of 1st 10 emails: [22, 35, 19, 4, 12, 58, 31, 11, 14, 18]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"🔎 Number of sentences of 1st 10 emails: %s\" % f1[:10])\n",
    "not spelling_dictionary.check('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F2: The number of verbs in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = [len(list(filter(lambda word_tagged: word_tagged[1] == 'VB', document_word_tagged))) for document_word_tagged in documents_word_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Number of verbs in 1st 10 emails: [6, 4, 10, 3, 10, 30, 11, 8, 16, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"🔎 Number of verbs in 1st 10 emails: %s\" % f2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F3: The number of words containing both numeric and alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = [len(list(filter(lambda word: re.search(r\"[a-zA-Z]+\", word) and re.search(r\"[1-9]+\", word), document_word_tokenized))) for document_word_tokenized in documents_word_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Number of words in 1st 10 emails containing alphabetic & numeric characters: [0, 0, 0, 0, 0, 1, 2, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"🔎 Number of words in 1st 10 emails containing alphabetic & numeric characters: %s\" % f3[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F4: The number of words in an email that are found in the spam list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f4 = [reduce(lambda accumulative, spam_term: accumulative + len(re.findall(r\"\\b\" + spam_term + r\"\\b\", document.lower())), spam_term_list, 0) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Number of words in 1st 10 emails that are found in the spam list: [3, 2, 3, 2, 4, 8, 4, 3, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"🔎 Number of words in 1st 10 emails that are found in the spam list: %s\" % f4[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F5: The number of words in an email that have more than 3 syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f5 = list(map(lambda counts: len(counts), [list(filter(lambda count: count > 3, document_syllabafied)) for document_syllabafied in documents_syllabafied]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Number of words in 1st 10 emails that have more than 3 syllables: [30, 11, 5, 1, 4, 35, 10, 4, 4, 13]\n"
     ]
    }
   ],
   "source": [
    "print(\"🔎 Number of words in 1st 10 emails that have more than 3 syllables: %s\" % f5[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F6: The average number of syllables of words in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f6 = list(map(lambda counts: numpy.mean(counts), documents_syllabafied))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Average number of syllables of words: [1.6418918918918919, 1.5289017341040463, 1.4152249134948096, 1.3650793650793651, 1.2658227848101267, 1.4833524684270953, 1.4852941176470589, 1.5030674846625767, 1.288, 1.5103448275862068]\n"
     ]
    }
   ],
   "source": [
    "print(\"🔎 Average number of syllables of words: %s\" % f6[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F7: The number of spelling mistakes in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f7 = [len(list(filter(lambda word: not spelling_dictionary.check(word), document_word_tokenized))) for document_word_tokenized in documents_word_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Number of misspelled words: [252, 97, 67, 6, 63, 262, 83, 56, 35, 100]\n"
     ]
    }
   ],
   "source": [
    "print(\"🔎 Number of misspelled words: %s\" % f7[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a feature matrix (list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Feature matrix of 1st document: [22, 6, 0, 3, 30, 1.6418918918918919, 252]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = [[f1[i], f2[i], f3[i], f4[i], f5[i], f6[i], f7[i]] for i in range(len(documents))]\n",
    "print(\"🔎 Feature matrix of 1st document: %s\" % feature_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed the feature matrix and the labels to any of the sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Naive Bayes classifier precision score: 0.834477\n",
      "🔎 Naive Bayes classifier recall score: 0.877297\n",
      "🔎 Naive Bayes classifier f-score: 0.853492\n"
     ]
    }
   ],
   "source": [
    "## seperate feature matrix to training and test sets\n",
    "feature_train = feature_matrix[:training_documents_count]\n",
    "feature_test = feature_matrix[training_documents_count:]\n",
    "\n",
    "## feed into a classifier\n",
    "naive_bayes_classifier = MultinomialNB(alpha=1)\n",
    "naive_bayes_classifier.fit(feature_train, training_labels)\n",
    "\n",
    "naive_bayes_classifier_predictions = naive_bayes_classifier.predict(feature_test)\n",
    "\n",
    "naive_bayes_classifier_precision_score = metrics.precision_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_recall_score = metrics.recall_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_f1_score = metrics.f1_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 Naive Bayes classifier precision score: %f\" % naive_bayes_classifier_precision_score)\n",
    "print(\"🔎 Naive Bayes classifier recall score: %f\" % naive_bayes_classifier_recall_score)\n",
    "print(\"🔎 Naive Bayes classifier f-score: %f\" % naive_bayes_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 K Neighbors classifier precision score: 0.902082\n",
      "🔎 K Neighbors classifier recall score: 0.774004\n",
      "🔎 K Neighbors classifier f-score: 0.819474\n"
     ]
    }
   ],
   "source": [
    "kneighbors_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "kneighbors_classifier.fit(feature_train, training_labels)\n",
    "\n",
    "kneighbors_classifier_predictions = kneighbors_classifier.predict(feature_test)\n",
    "\n",
    "kneighbors_classifier_precision_score = metrics.precision_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_recall_score = metrics.recall_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_f1_score = metrics.f1_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 K Neighbors classifier precision score: %f\" % kneighbors_classifier_precision_score)\n",
    "print(\"🔎 K Neighbors classifier recall score: %f\" % kneighbors_classifier_recall_score)\n",
    "print(\"🔎 K Neighbors classifier f-score: %f\" % kneighbors_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Random Forest classifier precision score: 0.942254\n",
      "🔎 Random Forest classifier recall score: 0.864616\n",
      "🔎 Random Forest classifier f-score: 0.897578\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=0)\n",
    "random_forest_classifier.fit(feature_train, training_labels)\n",
    "\n",
    "random_forest_classifier_predictions = random_forest_classifier.predict(feature_test)\n",
    "\n",
    "random_forest_classifier_precision_score = metrics.precision_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_recall_score = metrics.recall_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_f1_score = metrics.f1_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 Random Forest classifier precision score: %f\" % random_forest_classifier_precision_score)\n",
    "print(\"🔎 Random Forest classifier recall score: %f\" % random_forest_classifier_recall_score)\n",
    "print(\"🔎 Random Forest classifier f-score: %f\" % random_forest_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: X9: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(training_documents)\n",
    "\n",
    "training_tfidf = tfidf_vectorizer.transform(training_documents)\n",
    "testing_tfidf = tfidf_vectorizer.transform(testing_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Naive Bayes classifier precision score: 0.920732\n",
      "🔎 Naive Bayes classifier recall score: 0.526042\n",
      "🔎 Naive Bayes classifier f-score: 0.506459\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = MultinomialNB(alpha=1)\n",
    "naive_bayes_classifier.fit(training_tfidf, training_labels)\n",
    "\n",
    "naive_bayes_classifier_predictions = naive_bayes_classifier.predict(testing_tfidf)\n",
    "\n",
    "naive_bayes_classifier_precision_score = metrics.precision_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_recall_score = metrics.recall_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_f1_score = metrics.f1_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 Naive Bayes classifier precision score: %f\" % naive_bayes_classifier_precision_score)\n",
    "print(\"🔎 Naive Bayes classifier recall score: %f\" % naive_bayes_classifier_recall_score)\n",
    "print(\"🔎 Naive Bayes classifier f-score: %f\" % naive_bayes_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 K Neighbors classifier precision score: 0.942866\n",
      "🔎 K Neighbors classifier recall score: 0.982369\n",
      "🔎 K Neighbors classifier f-score: 0.961174\n"
     ]
    }
   ],
   "source": [
    "kneighbors_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "kneighbors_classifier.fit(training_tfidf, training_labels)\n",
    "\n",
    "kneighbors_classifier_predictions = kneighbors_classifier.predict(testing_tfidf)\n",
    "\n",
    "kneighbors_classifier_precision_score = metrics.precision_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_recall_score = metrics.recall_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_f1_score = metrics.f1_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 K Neighbors classifier precision score: %f\" % kneighbors_classifier_precision_score)\n",
    "print(\"🔎 K Neighbors classifier recall score: %f\" % kneighbors_classifier_recall_score)\n",
    "print(\"🔎 K Neighbors classifier f-score: %f\" % kneighbors_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Random Forest classifier precision score: 0.979167\n",
      "🔎 Random Forest classifier recall score: 0.890625\n",
      "🔎 Random Forest classifier f-score: 0.927958\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=0)\n",
    "random_forest_classifier.fit(training_tfidf, training_labels)\n",
    "\n",
    "random_forest_classifier_predictions = random_forest_classifier.predict(testing_tfidf)\n",
    "\n",
    "random_forest_classifier_precision_score = metrics.precision_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_recall_score = metrics.recall_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_f1_score = metrics.f1_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"🔎 Random Forest classifier precision score: %f\" % random_forest_classifier_precision_score)\n",
    "print(\"🔎 Random Forest classifier recall score: %f\" % random_forest_classifier_recall_score)\n",
    "print(\"🔎 Random Forest classifier f-score: %f\" % random_forest_classifier_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
