{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                    # for filesystem access\n",
    "import fnmatch                                               # for Unix filename pattern matching\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINGSPAM_BARE_DATASET_PATH = \"datasets/lingspam_public/bare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_spam_file_name(file_name):\n",
    "    return fnmatch.fnmatchcase(file_name, 'spmsg*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the emails in the ten folders & save the labels (spam/not spam, or 0/1) of each email to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, file_names in os.walk(LINGSPAM_BARE_DATASET_PATH):\n",
    "    for file_name in fnmatch.filter(file_names, '*.txt'):\n",
    "        with open(os.path.join(root, file_name), 'r') as file:\n",
    "            documents.append(file.read())\n",
    "            labels.append(1 if is_spam_file_name(file_name) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Read 2893 documents\n"
     ]
    }
   ],
   "source": [
    "documents_length = len(documents)\n",
    "\n",
    "if documents_length > 0:\n",
    "    print(\"✅ Read %i documents\" % len(documents))\n",
    "else:\n",
    "    print(\"❌ Could not read any documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the emails & labels into 80% training & 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents_count = round(documents_length * 0.8)\n",
    "\n",
    "training_documents = documents[:training_documents_count]\n",
    "training_labels = labels[:training_documents_count]\n",
    "\n",
    "testing_documents = documents[training_documents_count:]\n",
    "testing_labels = labels[training_documents_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the training emails & transform the testing emails using a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.fit(training_documents)\n",
    "\n",
    "training_document_term_matrix = count_vectorizer.transform(training_documents)\n",
    "testing_document_term_matrix = count_vectorizer.transform(testing_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Classifiers\n",
    "##### For each classifier, print the precision, recall and f-score on the testing data\n",
    "\n",
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "##### you can set random_state=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying using Readability Features\n",
    "\n",
    "Rather than using the whole text content of an email, some characteristic features can be extracted per email, that will be fed to the classifier. Extract some features. The features are:\n",
    "\n",
    "    a) F1: The number of sentences in an email.\n",
    "    b) F2: The number of verbs in an email.\n",
    "    c) F3: The number of words containing both numeric and alphabetical characters.\n",
    "    d) F4: The number of words in an email that are found in the spam list.\n",
    "    e) F5: The number of words in an email that have more than 3 syllables.\n",
    "    f) F6: The average number of syllables of words in an email.\n",
    "    \n",
    "For F2, you can find useful code in Lab Assignment 5 solution on the MET website. For F4, you will be checking how many words in a given email are found in a spam word-list. The word-list you will be using can be found here. For F5 and F6, you can use the library Pyphen (with lang=’en_GB’).\n",
    "\n",
    "The steps are:\n",
    "\n",
    "    a) Create a list for every feature, where every element is the feature value of a given email (or use a\n",
    "    dictionary, key is feature name, value is feature list).\n",
    "    b) Build a feature matrix (list of lists), where every row corresponds to an email, and every column\n",
    "    corresponds to a feature value of this email.\n",
    "    c) Feed the feature matrix and the labels to any of the sklearn classifiers.\n",
    "    \n",
    "On the MET website, you will find a file titled “feature-construction”. This is an example of building a\n",
    "feature matrix (steps “a” and “b”). Note that this is just a sample, the documents and the features to be\n",
    "extracted will be different in the project.\n",
    "\n",
    "##### For the classifier, print the precision, recall and f-score on the testing data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
